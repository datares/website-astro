---
layout: ../../layouts/SimpleLayout.astro
title: Research
subtitle: Our team investigates a range of advanced machine learning problems primarily involving deep learning and reinforcement learning.
---

# Who we are

The research team at DataRes is a group of highly motivated students, all with an interest in machine learning. We strive to explore novel ideas in this quickly developing field.

From large language models to geometric deep learning, we develop knowledge about and research cutting edge algorithms.

If any of this sounds interesting to you, take a look at past work, and consider applying!

# Previous work

In the 2019-2020 academic year, we worked with [reinforcement learning](<https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning.>)
and [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning#:~:text=Supervised%20learning%20is%20the%20machine,a%20set%20of%20training%20examples.) problems,
which required our members to apply their software engineering, research and critical thinking skills.

The first problem, which you can read about [here](https://medium.com/@ucladatares/rlette-casino-roulette-through-reinforcement-learning-67e865843f0d), was to teach an agent how to gamble in the game of Roulette.

Eventually, our members were able to learn from the agent, and understand they should in fact
not gamble at all, as most betting positions are far from being statistically convenient for the gambler.

With the second problem, we took it a step further. We taught our members what a [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) is, and how we can use it
for regression, computer vision problems.

Specifically, [we taught an AI how to predict speed from just video frames](https://medium.com/@ucladatares/predicting-speed-from-video-frames-dissecting-the-comma-ai-challenge-5da697b55886).
This included a lot of preprocessing, one of the most fundamental steps in data science, which allowed our members
to learn about the hassles of training a realistic model that generalizes well enough.

Subsequently, in 2022-2023, we focused on transformer models and graph neural networks.

With regards to transfomer models, we worked primarily with [BERT](https://arxiv.org/pdf/1810.04805.pdf), the language model that powers Google Search. Through fine tuning and modifications,
our teams were able to use BERT to process time series data, which included using BERT's power as an image transfomer (see [BEiT](https://arxiv.org/pdf/2106.08254.pdf)), and also to predict
oil prices over time using text-analysis of news articles.

In the graph neural network space, our teams utilized [PyTorch Geometric](https://www.pyg.org/), a powerful geometric deep learning framework based on PyTorch.
Using these nascent ideas in deep learning, they were able to explore the possiblities of representation learning, drawing out information from complex, relational datasets,
such as Wikipedia articles, and road networks.

# Apply, to learn, to grow, to make a change.

We are excited to meet you, learn from you, and help you grow.
If you have any questions, feel free to email [Lukas Brockenbrough](mailto:lukasbroc@gmail.com) or [Sammy Shang](mailto:sammy7shang@ucla.edu).

Cheers.
